// Code generated by pulumi-language-go DO NOT EDIT.
// *** WARNING: Do not edit by hand unless you're certain you know what you are doing! ***

package google

import (
	"context"
	"reflect"

	"errors"
	"github.com/pulumi/pulumi-terraform-provider/sdks/go/google/v6/google/internal"
	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
)

type DataprocJob struct {
	pulumi.CustomResourceState

	DataprocJobId pulumi.StringOutput `pulumi:"dataprocJobId"`
	// Output-only. If present, the location of miscellaneous control files which may be used as part of job setup and
	// handling. If not present, control files may be placed in the same location as driver_output_uri.
	DriverControlsFilesUri pulumi.StringOutput `pulumi:"driverControlsFilesUri"`
	// Output-only. A URI pointing to the location of the stdout of the job's driver program
	DriverOutputResourceUri pulumi.StringOutput    `pulumi:"driverOutputResourceUri"`
	EffectiveLabels         pulumi.StringMapOutput `pulumi:"effectiveLabels"`
	// By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure
	// that the job is first cancelled before issuing the delete.
	ForceDelete pulumi.BoolPtrOutput `pulumi:"forceDelete"`
	// The config of Hadoop job
	HadoopConfig DataprocJobHadoopConfigPtrOutput `pulumi:"hadoopConfig"`
	// The config of hive job
	HiveConfig DataprocJobHiveConfigPtrOutput `pulumi:"hiveConfig"`
	// Optional. The labels to associate with this job. **Note**: This field is non-authoritative, and will only manage the
	// labels present in your configuration. Please refer to the field 'effective_labels' for all of the labels present on the
	// resource.
	Labels pulumi.StringMapOutput `pulumi:"labels"`
	// The config of pag job.
	PigConfig DataprocJobPigConfigPtrOutput `pulumi:"pigConfig"`
	// The config of job placement.
	Placement DataprocJobPlacementOutput `pulumi:"placement"`
	// The config of presto job
	PrestoConfig DataprocJobPrestoConfigPtrOutput `pulumi:"prestoConfig"`
	// The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider
	// project is used.
	Project pulumi.StringOutput `pulumi:"project"`
	// The config of pySpark job.
	PysparkConfig DataprocJobPysparkConfigPtrOutput `pulumi:"pysparkConfig"`
	// The reference of the job
	Reference DataprocJobReferencePtrOutput `pulumi:"reference"`
	// The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If
	// not specified, defaults to global.
	Region pulumi.StringPtrOutput `pulumi:"region"`
	// Optional. Job scheduling configuration.
	Scheduling DataprocJobSchedulingPtrOutput `pulumi:"scheduling"`
	// The config of the Spark job.
	SparkConfig DataprocJobSparkConfigPtrOutput `pulumi:"sparkConfig"`
	// The config of SparkSql job
	SparksqlConfig DataprocJobSparksqlConfigPtrOutput `pulumi:"sparksqlConfig"`
	// The status of the job.
	Statuses DataprocJobStatusArrayOutput `pulumi:"statuses"`
	// The combination of labels configured directly on the resource and default labels configured on the provider.
	TerraformLabels pulumi.StringMapOutput       `pulumi:"terraformLabels"`
	Timeouts        DataprocJobTimeoutsPtrOutput `pulumi:"timeouts"`
}

// NewDataprocJob registers a new resource with the given unique name, arguments, and options.
func NewDataprocJob(ctx *pulumi.Context,
	name string, args *DataprocJobArgs, opts ...pulumi.ResourceOption) (*DataprocJob, error) {
	if args == nil {
		return nil, errors.New("missing one or more required arguments")
	}

	if args.Placement == nil {
		return nil, errors.New("invalid value for required argument 'Placement'")
	}
	opts = internal.PkgResourceDefaultOpts(opts)
	ref, err := internal.PkgGetPackageRef(ctx)
	if err != nil {
		return nil, err
	}
	var resource DataprocJob
	err = ctx.RegisterPackageResource("google:index/dataprocJob:DataprocJob", name, args, &resource, ref, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// GetDataprocJob gets an existing DataprocJob resource's state with the given name, ID, and optional
// state properties that are used to uniquely qualify the lookup (nil if not required).
func GetDataprocJob(ctx *pulumi.Context,
	name string, id pulumi.IDInput, state *DataprocJobState, opts ...pulumi.ResourceOption) (*DataprocJob, error) {
	var resource DataprocJob
	ref, err := internal.PkgGetPackageRef(ctx)
	if err != nil {
		return nil, err
	}
	err = ctx.ReadPackageResource("google:index/dataprocJob:DataprocJob", name, id, state, &resource, ref, opts...)
	if err != nil {
		return nil, err
	}
	return &resource, nil
}

// Input properties used for looking up and filtering DataprocJob resources.
type dataprocJobState struct {
	DataprocJobId *string `pulumi:"dataprocJobId"`
	// Output-only. If present, the location of miscellaneous control files which may be used as part of job setup and
	// handling. If not present, control files may be placed in the same location as driver_output_uri.
	DriverControlsFilesUri *string `pulumi:"driverControlsFilesUri"`
	// Output-only. A URI pointing to the location of the stdout of the job's driver program
	DriverOutputResourceUri *string           `pulumi:"driverOutputResourceUri"`
	EffectiveLabels         map[string]string `pulumi:"effectiveLabels"`
	// By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure
	// that the job is first cancelled before issuing the delete.
	ForceDelete *bool `pulumi:"forceDelete"`
	// The config of Hadoop job
	HadoopConfig *DataprocJobHadoopConfig `pulumi:"hadoopConfig"`
	// The config of hive job
	HiveConfig *DataprocJobHiveConfig `pulumi:"hiveConfig"`
	// Optional. The labels to associate with this job. **Note**: This field is non-authoritative, and will only manage the
	// labels present in your configuration. Please refer to the field 'effective_labels' for all of the labels present on the
	// resource.
	Labels map[string]string `pulumi:"labels"`
	// The config of pag job.
	PigConfig *DataprocJobPigConfig `pulumi:"pigConfig"`
	// The config of job placement.
	Placement *DataprocJobPlacement `pulumi:"placement"`
	// The config of presto job
	PrestoConfig *DataprocJobPrestoConfig `pulumi:"prestoConfig"`
	// The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider
	// project is used.
	Project *string `pulumi:"project"`
	// The config of pySpark job.
	PysparkConfig *DataprocJobPysparkConfig `pulumi:"pysparkConfig"`
	// The reference of the job
	Reference *DataprocJobReference `pulumi:"reference"`
	// The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If
	// not specified, defaults to global.
	Region *string `pulumi:"region"`
	// Optional. Job scheduling configuration.
	Scheduling *DataprocJobScheduling `pulumi:"scheduling"`
	// The config of the Spark job.
	SparkConfig *DataprocJobSparkConfig `pulumi:"sparkConfig"`
	// The config of SparkSql job
	SparksqlConfig *DataprocJobSparksqlConfig `pulumi:"sparksqlConfig"`
	// The status of the job.
	Statuses []DataprocJobStatus `pulumi:"statuses"`
	// The combination of labels configured directly on the resource and default labels configured on the provider.
	TerraformLabels map[string]string    `pulumi:"terraformLabels"`
	Timeouts        *DataprocJobTimeouts `pulumi:"timeouts"`
}

type DataprocJobState struct {
	DataprocJobId pulumi.StringPtrInput
	// Output-only. If present, the location of miscellaneous control files which may be used as part of job setup and
	// handling. If not present, control files may be placed in the same location as driver_output_uri.
	DriverControlsFilesUri pulumi.StringPtrInput
	// Output-only. A URI pointing to the location of the stdout of the job's driver program
	DriverOutputResourceUri pulumi.StringPtrInput
	EffectiveLabels         pulumi.StringMapInput
	// By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure
	// that the job is first cancelled before issuing the delete.
	ForceDelete pulumi.BoolPtrInput
	// The config of Hadoop job
	HadoopConfig DataprocJobHadoopConfigPtrInput
	// The config of hive job
	HiveConfig DataprocJobHiveConfigPtrInput
	// Optional. The labels to associate with this job. **Note**: This field is non-authoritative, and will only manage the
	// labels present in your configuration. Please refer to the field 'effective_labels' for all of the labels present on the
	// resource.
	Labels pulumi.StringMapInput
	// The config of pag job.
	PigConfig DataprocJobPigConfigPtrInput
	// The config of job placement.
	Placement DataprocJobPlacementPtrInput
	// The config of presto job
	PrestoConfig DataprocJobPrestoConfigPtrInput
	// The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider
	// project is used.
	Project pulumi.StringPtrInput
	// The config of pySpark job.
	PysparkConfig DataprocJobPysparkConfigPtrInput
	// The reference of the job
	Reference DataprocJobReferencePtrInput
	// The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If
	// not specified, defaults to global.
	Region pulumi.StringPtrInput
	// Optional. Job scheduling configuration.
	Scheduling DataprocJobSchedulingPtrInput
	// The config of the Spark job.
	SparkConfig DataprocJobSparkConfigPtrInput
	// The config of SparkSql job
	SparksqlConfig DataprocJobSparksqlConfigPtrInput
	// The status of the job.
	Statuses DataprocJobStatusArrayInput
	// The combination of labels configured directly on the resource and default labels configured on the provider.
	TerraformLabels pulumi.StringMapInput
	Timeouts        DataprocJobTimeoutsPtrInput
}

func (DataprocJobState) ElementType() reflect.Type {
	return reflect.TypeOf((*dataprocJobState)(nil)).Elem()
}

type dataprocJobArgs struct {
	DataprocJobId *string `pulumi:"dataprocJobId"`
	// By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure
	// that the job is first cancelled before issuing the delete.
	ForceDelete *bool `pulumi:"forceDelete"`
	// The config of Hadoop job
	HadoopConfig *DataprocJobHadoopConfig `pulumi:"hadoopConfig"`
	// The config of hive job
	HiveConfig *DataprocJobHiveConfig `pulumi:"hiveConfig"`
	// Optional. The labels to associate with this job. **Note**: This field is non-authoritative, and will only manage the
	// labels present in your configuration. Please refer to the field 'effective_labels' for all of the labels present on the
	// resource.
	Labels map[string]string `pulumi:"labels"`
	// The config of pag job.
	PigConfig *DataprocJobPigConfig `pulumi:"pigConfig"`
	// The config of job placement.
	Placement DataprocJobPlacement `pulumi:"placement"`
	// The config of presto job
	PrestoConfig *DataprocJobPrestoConfig `pulumi:"prestoConfig"`
	// The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider
	// project is used.
	Project *string `pulumi:"project"`
	// The config of pySpark job.
	PysparkConfig *DataprocJobPysparkConfig `pulumi:"pysparkConfig"`
	// The reference of the job
	Reference *DataprocJobReference `pulumi:"reference"`
	// The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If
	// not specified, defaults to global.
	Region *string `pulumi:"region"`
	// Optional. Job scheduling configuration.
	Scheduling *DataprocJobScheduling `pulumi:"scheduling"`
	// The config of the Spark job.
	SparkConfig *DataprocJobSparkConfig `pulumi:"sparkConfig"`
	// The config of SparkSql job
	SparksqlConfig *DataprocJobSparksqlConfig `pulumi:"sparksqlConfig"`
	Timeouts       *DataprocJobTimeouts       `pulumi:"timeouts"`
}

// The set of arguments for constructing a DataprocJob resource.
type DataprocJobArgs struct {
	DataprocJobId pulumi.StringPtrInput
	// By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure
	// that the job is first cancelled before issuing the delete.
	ForceDelete pulumi.BoolPtrInput
	// The config of Hadoop job
	HadoopConfig DataprocJobHadoopConfigPtrInput
	// The config of hive job
	HiveConfig DataprocJobHiveConfigPtrInput
	// Optional. The labels to associate with this job. **Note**: This field is non-authoritative, and will only manage the
	// labels present in your configuration. Please refer to the field 'effective_labels' for all of the labels present on the
	// resource.
	Labels pulumi.StringMapInput
	// The config of pag job.
	PigConfig DataprocJobPigConfigPtrInput
	// The config of job placement.
	Placement DataprocJobPlacementInput
	// The config of presto job
	PrestoConfig DataprocJobPrestoConfigPtrInput
	// The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider
	// project is used.
	Project pulumi.StringPtrInput
	// The config of pySpark job.
	PysparkConfig DataprocJobPysparkConfigPtrInput
	// The reference of the job
	Reference DataprocJobReferencePtrInput
	// The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If
	// not specified, defaults to global.
	Region pulumi.StringPtrInput
	// Optional. Job scheduling configuration.
	Scheduling DataprocJobSchedulingPtrInput
	// The config of the Spark job.
	SparkConfig DataprocJobSparkConfigPtrInput
	// The config of SparkSql job
	SparksqlConfig DataprocJobSparksqlConfigPtrInput
	Timeouts       DataprocJobTimeoutsPtrInput
}

func (DataprocJobArgs) ElementType() reflect.Type {
	return reflect.TypeOf((*dataprocJobArgs)(nil)).Elem()
}

type DataprocJobInput interface {
	pulumi.Input

	ToDataprocJobOutput() DataprocJobOutput
	ToDataprocJobOutputWithContext(ctx context.Context) DataprocJobOutput
}

func (*DataprocJob) ElementType() reflect.Type {
	return reflect.TypeOf((**DataprocJob)(nil)).Elem()
}

func (i *DataprocJob) ToDataprocJobOutput() DataprocJobOutput {
	return i.ToDataprocJobOutputWithContext(context.Background())
}

func (i *DataprocJob) ToDataprocJobOutputWithContext(ctx context.Context) DataprocJobOutput {
	return pulumi.ToOutputWithContext(ctx, i).(DataprocJobOutput)
}

type DataprocJobOutput struct{ *pulumi.OutputState }

func (DataprocJobOutput) ElementType() reflect.Type {
	return reflect.TypeOf((**DataprocJob)(nil)).Elem()
}

func (o DataprocJobOutput) ToDataprocJobOutput() DataprocJobOutput {
	return o
}

func (o DataprocJobOutput) ToDataprocJobOutputWithContext(ctx context.Context) DataprocJobOutput {
	return o
}

func (o DataprocJobOutput) DataprocJobId() pulumi.StringOutput {
	return o.ApplyT(func(v *DataprocJob) pulumi.StringOutput { return v.DataprocJobId }).(pulumi.StringOutput)
}

// Output-only. If present, the location of miscellaneous control files which may be used as part of job setup and
// handling. If not present, control files may be placed in the same location as driver_output_uri.
func (o DataprocJobOutput) DriverControlsFilesUri() pulumi.StringOutput {
	return o.ApplyT(func(v *DataprocJob) pulumi.StringOutput { return v.DriverControlsFilesUri }).(pulumi.StringOutput)
}

// Output-only. A URI pointing to the location of the stdout of the job's driver program
func (o DataprocJobOutput) DriverOutputResourceUri() pulumi.StringOutput {
	return o.ApplyT(func(v *DataprocJob) pulumi.StringOutput { return v.DriverOutputResourceUri }).(pulumi.StringOutput)
}

func (o DataprocJobOutput) EffectiveLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *DataprocJob) pulumi.StringMapOutput { return v.EffectiveLabels }).(pulumi.StringMapOutput)
}

// By default, you can only delete inactive jobs within Dataproc. Setting this to true, and calling destroy, will ensure
// that the job is first cancelled before issuing the delete.
func (o DataprocJobOutput) ForceDelete() pulumi.BoolPtrOutput {
	return o.ApplyT(func(v *DataprocJob) pulumi.BoolPtrOutput { return v.ForceDelete }).(pulumi.BoolPtrOutput)
}

// The config of Hadoop job
func (o DataprocJobOutput) HadoopConfig() DataprocJobHadoopConfigPtrOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobHadoopConfigPtrOutput { return v.HadoopConfig }).(DataprocJobHadoopConfigPtrOutput)
}

// The config of hive job
func (o DataprocJobOutput) HiveConfig() DataprocJobHiveConfigPtrOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobHiveConfigPtrOutput { return v.HiveConfig }).(DataprocJobHiveConfigPtrOutput)
}

// Optional. The labels to associate with this job. **Note**: This field is non-authoritative, and will only manage the
// labels present in your configuration. Please refer to the field 'effective_labels' for all of the labels present on the
// resource.
func (o DataprocJobOutput) Labels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *DataprocJob) pulumi.StringMapOutput { return v.Labels }).(pulumi.StringMapOutput)
}

// The config of pag job.
func (o DataprocJobOutput) PigConfig() DataprocJobPigConfigPtrOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobPigConfigPtrOutput { return v.PigConfig }).(DataprocJobPigConfigPtrOutput)
}

// The config of job placement.
func (o DataprocJobOutput) Placement() DataprocJobPlacementOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobPlacementOutput { return v.Placement }).(DataprocJobPlacementOutput)
}

// The config of presto job
func (o DataprocJobOutput) PrestoConfig() DataprocJobPrestoConfigPtrOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobPrestoConfigPtrOutput { return v.PrestoConfig }).(DataprocJobPrestoConfigPtrOutput)
}

// The project in which the cluster can be found and jobs subsequently run against. If it is not provided, the provider
// project is used.
func (o DataprocJobOutput) Project() pulumi.StringOutput {
	return o.ApplyT(func(v *DataprocJob) pulumi.StringOutput { return v.Project }).(pulumi.StringOutput)
}

// The config of pySpark job.
func (o DataprocJobOutput) PysparkConfig() DataprocJobPysparkConfigPtrOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobPysparkConfigPtrOutput { return v.PysparkConfig }).(DataprocJobPysparkConfigPtrOutput)
}

// The reference of the job
func (o DataprocJobOutput) Reference() DataprocJobReferencePtrOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobReferencePtrOutput { return v.Reference }).(DataprocJobReferencePtrOutput)
}

// The Cloud Dataproc region. This essentially determines which clusters are available for this job to be submitted to. If
// not specified, defaults to global.
func (o DataprocJobOutput) Region() pulumi.StringPtrOutput {
	return o.ApplyT(func(v *DataprocJob) pulumi.StringPtrOutput { return v.Region }).(pulumi.StringPtrOutput)
}

// Optional. Job scheduling configuration.
func (o DataprocJobOutput) Scheduling() DataprocJobSchedulingPtrOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobSchedulingPtrOutput { return v.Scheduling }).(DataprocJobSchedulingPtrOutput)
}

// The config of the Spark job.
func (o DataprocJobOutput) SparkConfig() DataprocJobSparkConfigPtrOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobSparkConfigPtrOutput { return v.SparkConfig }).(DataprocJobSparkConfigPtrOutput)
}

// The config of SparkSql job
func (o DataprocJobOutput) SparksqlConfig() DataprocJobSparksqlConfigPtrOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobSparksqlConfigPtrOutput { return v.SparksqlConfig }).(DataprocJobSparksqlConfigPtrOutput)
}

// The status of the job.
func (o DataprocJobOutput) Statuses() DataprocJobStatusArrayOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobStatusArrayOutput { return v.Statuses }).(DataprocJobStatusArrayOutput)
}

// The combination of labels configured directly on the resource and default labels configured on the provider.
func (o DataprocJobOutput) TerraformLabels() pulumi.StringMapOutput {
	return o.ApplyT(func(v *DataprocJob) pulumi.StringMapOutput { return v.TerraformLabels }).(pulumi.StringMapOutput)
}

func (o DataprocJobOutput) Timeouts() DataprocJobTimeoutsPtrOutput {
	return o.ApplyT(func(v *DataprocJob) DataprocJobTimeoutsPtrOutput { return v.Timeouts }).(DataprocJobTimeoutsPtrOutput)
}

func init() {
	pulumi.RegisterInputType(reflect.TypeOf((*DataprocJobInput)(nil)).Elem(), &DataprocJob{})
	pulumi.RegisterOutputType(DataprocJobOutput{})
}
